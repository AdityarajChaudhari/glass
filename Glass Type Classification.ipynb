{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Type\n",
       "0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0     1\n",
       "1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0     1\n",
       "2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0     1\n",
       "3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0     1\n",
       "4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'D:\\Datasets\\glass.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2140"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 214 entries, 0 to 213\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   RI      214 non-null    float64\n",
      " 1   Na      214 non-null    float64\n",
      " 2   Mg      214 non-null    float64\n",
      " 3   Al      214 non-null    float64\n",
      " 4   Si      214 non-null    float64\n",
      " 5   K       214 non-null    float64\n",
      " 6   Ca      214 non-null    float64\n",
      " 7   Ba      214 non-null    float64\n",
      " 8   Fe      214 non-null    float64\n",
      " 9   Type    214 non-null    int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 16.8 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.518365</td>\n",
       "      <td>13.407850</td>\n",
       "      <td>2.684533</td>\n",
       "      <td>1.444907</td>\n",
       "      <td>72.650935</td>\n",
       "      <td>0.497056</td>\n",
       "      <td>8.956963</td>\n",
       "      <td>0.175047</td>\n",
       "      <td>0.057009</td>\n",
       "      <td>2.780374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.816604</td>\n",
       "      <td>1.442408</td>\n",
       "      <td>0.499270</td>\n",
       "      <td>0.774546</td>\n",
       "      <td>0.652192</td>\n",
       "      <td>1.423153</td>\n",
       "      <td>0.497219</td>\n",
       "      <td>0.097439</td>\n",
       "      <td>2.103739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.511150</td>\n",
       "      <td>10.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>69.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.430000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.516522</td>\n",
       "      <td>12.907500</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>8.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.517680</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>72.790000</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.519157</td>\n",
       "      <td>13.825000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>73.087500</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>9.172500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.533930</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>4.490000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>75.410000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>16.190000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               RI          Na          Mg          Al          Si           K  \\\n",
       "count  214.000000  214.000000  214.000000  214.000000  214.000000  214.000000   \n",
       "mean     1.518365   13.407850    2.684533    1.444907   72.650935    0.497056   \n",
       "std      0.003037    0.816604    1.442408    0.499270    0.774546    0.652192   \n",
       "min      1.511150   10.730000    0.000000    0.290000   69.810000    0.000000   \n",
       "25%      1.516522   12.907500    2.115000    1.190000   72.280000    0.122500   \n",
       "50%      1.517680   13.300000    3.480000    1.360000   72.790000    0.555000   \n",
       "75%      1.519157   13.825000    3.600000    1.630000   73.087500    0.610000   \n",
       "max      1.533930   17.380000    4.490000    3.500000   75.410000    6.210000   \n",
       "\n",
       "               Ca          Ba          Fe        Type  \n",
       "count  214.000000  214.000000  214.000000  214.000000  \n",
       "mean     8.956963    0.175047    0.057009    2.780374  \n",
       "std      1.423153    0.497219    0.097439    2.103739  \n",
       "min      5.430000    0.000000    0.000000    1.000000  \n",
       "25%      8.240000    0.000000    0.000000    1.000000  \n",
       "50%      8.600000    0.000000    0.000000    2.000000  \n",
       "75%      9.172500    0.000000    0.100000    3.000000  \n",
       "max     16.190000    3.150000    0.510000    7.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    146\n",
       "1     68\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Type'] = np.where(data['Type']<3,0,1)\n",
    "data['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('Type',axis=1)\n",
    "y = data['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(292, 9) (292,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE()\n",
    "x_train_res,y_train_res = sm.fit_resample(x,y)\n",
    "print(x_train_res.shape, y_train_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_train_res,y_train_res,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = pd.DataFrame(scaler.fit_transform(x_train),columns=x_train.columns)\n",
    "x_test = pd.DataFrame(scaler.fit_transform(x_test),columns=x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8356164383561644\n",
      "[[36  1]\n",
      " [11 25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.97      0.86        37\n",
      "           1       0.96      0.69      0.81        36\n",
      "\n",
      "    accuracy                           0.84        73\n",
      "   macro avg       0.86      0.83      0.83        73\n",
      "weighted avg       0.86      0.84      0.83        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "print(metrics.confusion_matrix(y_test,y_pred))\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8447488584474886"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': ['l1', 'l2', 'elasticnet'],\n",
       " 'C': [0.0001,\n",
       "  0.001100900900900901,\n",
       "  0.0021018018018018015,\n",
       "  0.0031027027027027026,\n",
       "  0.004103603603603604,\n",
       "  0.005104504504504504,\n",
       "  0.006105405405405406,\n",
       "  0.0071063063063063064,\n",
       "  0.008107207207207206,\n",
       "  0.009108108108108108,\n",
       "  0.010109009009009007,\n",
       "  0.011109909909909909,\n",
       "  0.01211081081081081,\n",
       "  0.01311171171171171,\n",
       "  0.014112612612612612,\n",
       "  0.015113513513513512,\n",
       "  0.016114414414414413,\n",
       "  0.017115315315315315,\n",
       "  0.018116216216216216,\n",
       "  0.019117117117117114,\n",
       "  0.020118018018018016,\n",
       "  0.021118918918918917,\n",
       "  0.02211981981981982,\n",
       "  0.02312072072072072,\n",
       "  0.02412162162162162,\n",
       "  0.02512252252252252,\n",
       "  0.02612342342342342,\n",
       "  0.027124324324324323,\n",
       "  0.028125225225225224,\n",
       "  0.029126126126126126,\n",
       "  0.030127027027027024,\n",
       "  0.031127927927927925,\n",
       "  0.03212882882882883,\n",
       "  0.03312972972972973,\n",
       "  0.03413063063063063,\n",
       "  0.035131531531531535,\n",
       "  0.036132432432432436,\n",
       "  0.03713333333333334,\n",
       "  0.03813423423423423,\n",
       "  0.039135135135135134,\n",
       "  0.040136036036036035,\n",
       "  0.04113693693693694,\n",
       "  0.04213783783783784,\n",
       "  0.04313873873873874,\n",
       "  0.04413963963963964,\n",
       "  0.04514054054054054,\n",
       "  0.046141441441441444,\n",
       "  0.047142342342342346,\n",
       "  0.04814324324324325,\n",
       "  0.04914414414414414,\n",
       "  0.05014504504504504,\n",
       "  0.051145945945945945,\n",
       "  0.052146846846846846,\n",
       "  0.05314774774774775,\n",
       "  0.05414864864864865,\n",
       "  0.05514954954954955,\n",
       "  0.05615045045045045,\n",
       "  0.057151351351351354,\n",
       "  0.058152252252252255,\n",
       "  0.05915315315315315,\n",
       "  0.06015405405405405,\n",
       "  0.06115495495495495,\n",
       "  0.062155855855855854,\n",
       "  0.06315675675675676,\n",
       "  0.06415765765765766,\n",
       "  0.06515855855855855,\n",
       "  0.06615945945945946,\n",
       "  0.06716036036036035,\n",
       "  0.06816126126126126,\n",
       "  0.06916216216216216,\n",
       "  0.07016306306306307,\n",
       "  0.07116396396396396,\n",
       "  0.07216486486486487,\n",
       "  0.07316576576576576,\n",
       "  0.07416666666666667,\n",
       "  0.07516756756756757,\n",
       "  0.07616846846846846,\n",
       "  0.07716936936936937,\n",
       "  0.07817027027027026,\n",
       "  0.07917117117117117,\n",
       "  0.08017207207207207,\n",
       "  0.08117297297297298,\n",
       "  0.08217387387387387,\n",
       "  0.08317477477477478,\n",
       "  0.08417567567567567,\n",
       "  0.08517657657657658,\n",
       "  0.08617747747747748,\n",
       "  0.08717837837837837,\n",
       "  0.08817927927927928,\n",
       "  0.08918018018018017,\n",
       "  0.09018108108108108,\n",
       "  0.09118198198198198,\n",
       "  0.09218288288288289,\n",
       "  0.09318378378378378,\n",
       "  0.09418468468468469,\n",
       "  0.09518558558558558,\n",
       "  0.09618648648648649,\n",
       "  0.09718738738738739,\n",
       "  0.09818828828828828,\n",
       "  0.09918918918918919,\n",
       "  0.10019009009009008,\n",
       "  0.10119099099099099,\n",
       "  0.10219189189189189,\n",
       "  0.1031927927927928,\n",
       "  0.10419369369369369,\n",
       "  0.1051945945945946,\n",
       "  0.10619549549549549,\n",
       "  0.10719639639639639,\n",
       "  0.1081972972972973,\n",
       "  0.10919819819819819,\n",
       "  0.1101990990990991,\n",
       "  0.1112,\n",
       "  0.1122009009009009,\n",
       "  0.1132018018018018,\n",
       "  0.1142027027027027,\n",
       "  0.1152036036036036,\n",
       "  0.11620450450450451,\n",
       "  0.1172054054054054,\n",
       "  0.1182063063063063,\n",
       "  0.1192072072072072,\n",
       "  0.1202081081081081,\n",
       "  0.12120900900900901,\n",
       "  0.1222099099099099,\n",
       "  0.12321081081081081,\n",
       "  0.1242117117117117,\n",
       "  0.1252126126126126,\n",
       "  0.1262135135135135,\n",
       "  0.1272144144144144,\n",
       "  0.1282153153153153,\n",
       "  0.1292162162162162,\n",
       "  0.1302171171171171,\n",
       "  0.131218018018018,\n",
       "  0.1322189189189189,\n",
       "  0.1332198198198198,\n",
       "  0.1342207207207207,\n",
       "  0.1352216216216216,\n",
       "  0.1362225225225225,\n",
       "  0.13722342342342342,\n",
       "  0.1382243243243243,\n",
       "  0.1392252252252252,\n",
       "  0.14022612612612612,\n",
       "  0.141227027027027,\n",
       "  0.1422279279279279,\n",
       "  0.1432288288288288,\n",
       "  0.14422972972972972,\n",
       "  0.1452306306306306,\n",
       "  0.1462315315315315,\n",
       "  0.14723243243243242,\n",
       "  0.14823333333333333,\n",
       "  0.1492342342342342,\n",
       "  0.15023513513513512,\n",
       "  0.15123603603603603,\n",
       "  0.1522369369369369,\n",
       "  0.15323783783783781,\n",
       "  0.15423873873873872,\n",
       "  0.15523963963963963,\n",
       "  0.1562405405405405,\n",
       "  0.15724144144144142,\n",
       "  0.15824234234234233,\n",
       "  0.15924324324324324,\n",
       "  0.16024414414414412,\n",
       "  0.16124504504504503,\n",
       "  0.16224594594594594,\n",
       "  0.16324684684684682,\n",
       "  0.16424774774774772,\n",
       "  0.16524864864864863,\n",
       "  0.16624954954954954,\n",
       "  0.16725045045045042,\n",
       "  0.16825135135135133,\n",
       "  0.16925225225225224,\n",
       "  0.17025315315315315,\n",
       "  0.17125405405405403,\n",
       "  0.17225495495495494,\n",
       "  0.17325585585585584,\n",
       "  0.17425675675675673,\n",
       "  0.17525765765765763,\n",
       "  0.17625855855855854,\n",
       "  0.17725945945945945,\n",
       "  0.17826036036036033,\n",
       "  0.17926126126126124,\n",
       "  0.18026216216216215,\n",
       "  0.18126306306306306,\n",
       "  0.18226396396396394,\n",
       "  0.18326486486486485,\n",
       "  0.18426576576576575,\n",
       "  0.18526666666666664,\n",
       "  0.18626756756756754,\n",
       "  0.18726846846846845,\n",
       "  0.18826936936936936,\n",
       "  0.18927027027027024,\n",
       "  0.19027117117117115,\n",
       "  0.19127207207207206,\n",
       "  0.19227297297297297,\n",
       "  0.19327387387387385,\n",
       "  0.19427477477477476,\n",
       "  0.19527567567567566,\n",
       "  0.19627657657657654,\n",
       "  0.19727747747747745,\n",
       "  0.19827837837837836,\n",
       "  0.19927927927927927,\n",
       "  0.20028018018018015,\n",
       "  0.20128108108108106,\n",
       "  0.20228198198198197,\n",
       "  0.20328288288288285,\n",
       "  0.20428378378378376,\n",
       "  0.20528468468468467,\n",
       "  0.20628558558558557,\n",
       "  0.20728648648648645,\n",
       "  0.20828738738738736,\n",
       "  0.20928828828828827,\n",
       "  0.21028918918918918,\n",
       "  0.21129009009009006,\n",
       "  0.21229099099099097,\n",
       "  0.21329189189189188,\n",
       "  0.21429279279279276,\n",
       "  0.21529369369369367,\n",
       "  0.21629459459459457,\n",
       "  0.21729549549549548,\n",
       "  0.21829639639639636,\n",
       "  0.21929729729729727,\n",
       "  0.22029819819819818,\n",
       "  0.2212990990990991,\n",
       "  0.22229999999999997,\n",
       "  0.22330090090090088,\n",
       "  0.2243018018018018,\n",
       "  0.22530270270270267,\n",
       "  0.22630360360360358,\n",
       "  0.22730450450450448,\n",
       "  0.2283054054054054,\n",
       "  0.22930630630630627,\n",
       "  0.23030720720720718,\n",
       "  0.2313081081081081,\n",
       "  0.232309009009009,\n",
       "  0.23330990990990988,\n",
       "  0.2343108108108108,\n",
       "  0.2353117117117117,\n",
       "  0.23631261261261258,\n",
       "  0.23731351351351349,\n",
       "  0.2383144144144144,\n",
       "  0.2393153153153153,\n",
       "  0.24031621621621618,\n",
       "  0.2413171171171171,\n",
       "  0.242318018018018,\n",
       "  0.2433189189189189,\n",
       "  0.2443198198198198,\n",
       "  0.2453207207207207,\n",
       "  0.2463216216216216,\n",
       "  0.2473225225225225,\n",
       "  0.2483234234234234,\n",
       "  0.2493243243243243,\n",
       "  0.2503252252252252,\n",
       "  0.2513261261261261,\n",
       "  0.25232702702702703,\n",
       "  0.2533279279279279,\n",
       "  0.2543288288288288,\n",
       "  0.2553297297297297,\n",
       "  0.2563306306306306,\n",
       "  0.2573315315315315,\n",
       "  0.2583324324324324,\n",
       "  0.2593333333333333,\n",
       "  0.2603342342342342,\n",
       "  0.2613351351351351,\n",
       "  0.262336036036036,\n",
       "  0.26333693693693694,\n",
       "  0.2643378378378378,\n",
       "  0.2653387387387387,\n",
       "  0.26633963963963964,\n",
       "  0.2673405405405405,\n",
       "  0.2683414414414414,\n",
       "  0.26934234234234233,\n",
       "  0.2703432432432432,\n",
       "  0.2713441441441441,\n",
       "  0.27234504504504503,\n",
       "  0.2733459459459459,\n",
       "  0.27434684684684685,\n",
       "  0.27534774774774773,\n",
       "  0.2763486486486486,\n",
       "  0.27734954954954955,\n",
       "  0.2783504504504504,\n",
       "  0.2793513513513513,\n",
       "  0.28035225225225224,\n",
       "  0.2813531531531531,\n",
       "  0.282354054054054,\n",
       "  0.28335495495495494,\n",
       "  0.2843558558558558,\n",
       "  0.28535675675675676,\n",
       "  0.28635765765765764,\n",
       "  0.2873585585585585,\n",
       "  0.28835945945945946,\n",
       "  0.28936036036036034,\n",
       "  0.2903612612612612,\n",
       "  0.29136216216216215,\n",
       "  0.29236306306306303,\n",
       "  0.2933639639639639,\n",
       "  0.29436486486486485,\n",
       "  0.29536576576576573,\n",
       "  0.29636666666666667,\n",
       "  0.29736756756756755,\n",
       "  0.29836846846846843,\n",
       "  0.29936936936936936,\n",
       "  0.30037027027027025,\n",
       "  0.3013711711711711,\n",
       "  0.30237207207207206,\n",
       "  0.30337297297297294,\n",
       "  0.3043738738738738,\n",
       "  0.30537477477477476,\n",
       "  0.30637567567567564,\n",
       "  0.3073765765765766,\n",
       "  0.30837747747747746,\n",
       "  0.30937837837837834,\n",
       "  0.3103792792792793,\n",
       "  0.31138018018018015,\n",
       "  0.31238108108108104,\n",
       "  0.31338198198198197,\n",
       "  0.31438288288288285,\n",
       "  0.31538378378378373,\n",
       "  0.31638468468468467,\n",
       "  0.31738558558558555,\n",
       "  0.3183864864864865,\n",
       "  0.31938738738738737,\n",
       "  0.32038828828828825,\n",
       "  0.3213891891891892,\n",
       "  0.32239009009009006,\n",
       "  0.32339099099099095,\n",
       "  0.3243918918918919,\n",
       "  0.32539279279279276,\n",
       "  0.32639369369369364,\n",
       "  0.3273945945945946,\n",
       "  0.32839549549549546,\n",
       "  0.3293963963963964,\n",
       "  0.3303972972972973,\n",
       "  0.33139819819819816,\n",
       "  0.3323990990990991,\n",
       "  0.3334,\n",
       "  0.33440090090090085,\n",
       "  0.3354018018018018,\n",
       "  0.33640270270270267,\n",
       "  0.33740360360360355,\n",
       "  0.3384045045045045,\n",
       "  0.33940540540540537,\n",
       "  0.3404063063063063,\n",
       "  0.3414072072072072,\n",
       "  0.34240810810810807,\n",
       "  0.343409009009009,\n",
       "  0.3444099099099099,\n",
       "  0.34541081081081076,\n",
       "  0.3464117117117117,\n",
       "  0.3474126126126126,\n",
       "  0.34841351351351346,\n",
       "  0.3494144144144144,\n",
       "  0.3504153153153153,\n",
       "  0.3514162162162162,\n",
       "  0.3524171171171171,\n",
       "  0.353418018018018,\n",
       "  0.3544189189189189,\n",
       "  0.3554198198198198,\n",
       "  0.3564207207207207,\n",
       "  0.3574216216216216,\n",
       "  0.3584225225225225,\n",
       "  0.35942342342342337,\n",
       "  0.3604243243243243,\n",
       "  0.3614252252252252,\n",
       "  0.3624261261261261,\n",
       "  0.363427027027027,\n",
       "  0.3644279279279279,\n",
       "  0.3654288288288288,\n",
       "  0.3664297297297297,\n",
       "  0.3674306306306306,\n",
       "  0.3684315315315315,\n",
       "  0.3694324324324324,\n",
       "  0.3704333333333333,\n",
       "  0.3714342342342342,\n",
       "  0.3724351351351351,\n",
       "  0.37343603603603603,\n",
       "  0.3744369369369369,\n",
       "  0.3754378378378378,\n",
       "  0.37643873873873873,\n",
       "  0.3774396396396396,\n",
       "  0.3784405405405405,\n",
       "  0.37944144144144143,\n",
       "  0.3804423423423423,\n",
       "  0.3814432432432432,\n",
       "  0.3824441441441441,\n",
       "  0.383445045045045,\n",
       "  0.38444594594594594,\n",
       "  0.3854468468468468,\n",
       "  0.3864477477477477,\n",
       "  0.38744864864864864,\n",
       "  0.3884495495495495,\n",
       "  0.3894504504504504,\n",
       "  0.39045135135135134,\n",
       "  0.3914522522522522,\n",
       "  0.3924531531531531,\n",
       "  0.39345405405405404,\n",
       "  0.3944549549549549,\n",
       "  0.3954558558558558,\n",
       "  0.39645675675675673,\n",
       "  0.3974576576576576,\n",
       "  0.39845855855855855,\n",
       "  0.39945945945945943,\n",
       "  0.4004603603603603,\n",
       "  0.40146126126126125,\n",
       "  0.40246216216216213,\n",
       "  0.403463063063063,\n",
       "  0.40446396396396395,\n",
       "  0.4054648648648648,\n",
       "  0.4064657657657657,\n",
       "  0.40746666666666664,\n",
       "  0.4084675675675675,\n",
       "  0.40946846846846846,\n",
       "  0.41046936936936934,\n",
       "  0.4114702702702702,\n",
       "  0.41247117117117116,\n",
       "  0.41347207207207204,\n",
       "  0.4144729729729729,\n",
       "  0.41547387387387386,\n",
       "  0.41647477477477474,\n",
       "  0.4174756756756756,\n",
       "  0.41847657657657655,\n",
       "  0.41947747747747743,\n",
       "  0.42047837837837837,\n",
       "  0.42147927927927925,\n",
       "  0.42248018018018013,\n",
       "  0.42348108108108107,\n",
       "  0.42448198198198195,\n",
       "  0.42548288288288283,\n",
       "  0.42648378378378377,\n",
       "  0.42748468468468465,\n",
       "  0.4284855855855855,\n",
       "  0.42948648648648646,\n",
       "  0.43048738738738734,\n",
       "  0.4314882882882883,\n",
       "  0.43248918918918916,\n",
       "  0.43349009009009004,\n",
       "  0.434490990990991,\n",
       "  0.43549189189189186,\n",
       "  0.43649279279279274,\n",
       "  0.4374936936936937,\n",
       "  0.43849459459459456,\n",
       "  0.43949549549549544,\n",
       "  0.4404963963963964,\n",
       "  0.44149729729729725,\n",
       "  0.4424981981981982,\n",
       "  0.44349909909909907,\n",
       "  0.44449999999999995,\n",
       "  0.4455009009009009,\n",
       "  0.44650180180180177,\n",
       "  0.44750270270270265,\n",
       "  0.4485036036036036,\n",
       "  0.44950450450450447,\n",
       "  0.45050540540540535,\n",
       "  0.4515063063063063,\n",
       "  0.45250720720720716,\n",
       "  0.4535081081081081,\n",
       "  0.454509009009009,\n",
       "  0.45550990990990986,\n",
       "  0.4565108108108108,\n",
       "  0.4575117117117117,\n",
       "  0.45851261261261256,\n",
       "  0.4595135135135135,\n",
       "  0.4605144144144144,\n",
       "  0.46151531531531526,\n",
       "  0.4625162162162162,\n",
       "  0.4635171171171171,\n",
       "  0.464518018018018,\n",
       "  0.4655189189189189,\n",
       "  0.46651981981981977,\n",
       "  0.4675207207207207,\n",
       "  0.4685216216216216,\n",
       "  0.46952252252252247,\n",
       "  0.4705234234234234,\n",
       "  0.4715243243243243,\n",
       "  0.47252522522522517,\n",
       "  0.4735261261261261,\n",
       "  0.474527027027027,\n",
       "  0.4755279279279279,\n",
       "  0.4765288288288288,\n",
       "  0.4775297297297297,\n",
       "  0.4785306306306306,\n",
       "  0.4795315315315315,\n",
       "  0.4805324324324324,\n",
       "  0.4815333333333333,\n",
       "  0.4825342342342342,\n",
       "  0.4835351351351351,\n",
       "  0.484536036036036,\n",
       "  0.4855369369369369,\n",
       "  0.4865378378378378,\n",
       "  0.4875387387387387,\n",
       "  0.4885396396396396,\n",
       "  0.4895405405405405,\n",
       "  0.4905414414414414,\n",
       "  0.4915423423423423,\n",
       "  0.4925432432432432,\n",
       "  0.4935441441441441,\n",
       "  0.494545045045045,\n",
       "  0.4955459459459459,\n",
       "  0.4965468468468468,\n",
       "  0.49754774774774774,\n",
       "  0.4985486486486486,\n",
       "  0.4995495495495495,\n",
       "  0.5005504504504504,\n",
       "  0.5015513513513513,\n",
       "  0.5025522522522522,\n",
       "  0.5035531531531531,\n",
       "  0.5045540540540541,\n",
       "  0.505554954954955,\n",
       "  0.5065558558558558,\n",
       "  0.5075567567567567,\n",
       "  0.5085576576576576,\n",
       "  0.5095585585585585,\n",
       "  0.5105594594594595,\n",
       "  0.5115603603603603,\n",
       "  0.5125612612612612,\n",
       "  0.5135621621621621,\n",
       "  0.514563063063063,\n",
       "  0.515563963963964,\n",
       "  0.5165648648648649,\n",
       "  0.5175657657657657,\n",
       "  0.5185666666666666,\n",
       "  0.5195675675675675,\n",
       "  0.5205684684684684,\n",
       "  0.5215693693693694,\n",
       "  0.5225702702702703,\n",
       "  0.5235711711711711,\n",
       "  0.524572072072072,\n",
       "  0.5255729729729729,\n",
       "  0.5265738738738739,\n",
       "  0.5275747747747748,\n",
       "  0.5285756756756756,\n",
       "  0.5295765765765765,\n",
       "  0.5305774774774774,\n",
       "  0.5315783783783783,\n",
       "  0.5325792792792793,\n",
       "  0.5335801801801802,\n",
       "  0.534581081081081,\n",
       "  0.5355819819819819,\n",
       "  0.5365828828828828,\n",
       "  0.5375837837837838,\n",
       "  0.5385846846846847,\n",
       "  0.5395855855855856,\n",
       "  0.5405864864864864,\n",
       "  0.5415873873873873,\n",
       "  0.5425882882882882,\n",
       "  0.5435891891891892,\n",
       "  0.5445900900900901,\n",
       "  0.545590990990991,\n",
       "  0.5465918918918918,\n",
       "  0.5475927927927927,\n",
       "  0.5485936936936937,\n",
       "  0.5495945945945946,\n",
       "  0.5505954954954955,\n",
       "  0.5515963963963963,\n",
       "  0.5525972972972972,\n",
       "  0.5535981981981981,\n",
       "  0.5545990990990991,\n",
       "  0.5556,\n",
       "  0.5566009009009009,\n",
       "  0.5576018018018017,\n",
       "  0.5586027027027026,\n",
       "  0.5596036036036036,\n",
       "  0.5606045045045045,\n",
       "  0.5616054054054054,\n",
       "  0.5626063063063063,\n",
       "  0.5636072072072071,\n",
       "  0.564608108108108,\n",
       "  0.565609009009009,\n",
       "  0.5666099099099099,\n",
       "  0.5676108108108108,\n",
       "  0.5686117117117117,\n",
       "  0.5696126126126125,\n",
       "  0.5706135135135135,\n",
       "  0.5716144144144144,\n",
       "  0.5726153153153153,\n",
       "  0.5736162162162162,\n",
       "  0.574617117117117,\n",
       "  0.5756180180180179,\n",
       "  0.5766189189189189,\n",
       "  0.5776198198198198,\n",
       "  0.5786207207207207,\n",
       "  0.5796216216216216,\n",
       "  0.5806225225225224,\n",
       "  0.5816234234234234,\n",
       "  0.5826243243243243,\n",
       "  0.5836252252252252,\n",
       "  0.5846261261261261,\n",
       "  0.585627027027027,\n",
       "  0.5866279279279278,\n",
       "  0.5876288288288288,\n",
       "  0.5886297297297297,\n",
       "  0.5896306306306306,\n",
       "  0.5906315315315315,\n",
       "  0.5916324324324324,\n",
       "  0.5926333333333333,\n",
       "  0.5936342342342342,\n",
       "  0.5946351351351351,\n",
       "  0.595636036036036,\n",
       "  0.5966369369369369,\n",
       "  0.5976378378378377,\n",
       "  0.5986387387387387,\n",
       "  0.5996396396396396,\n",
       "  0.6006405405405405,\n",
       "  0.6016414414414414,\n",
       "  0.6026423423423423,\n",
       "  0.6036432432432433,\n",
       "  0.6046441441441441,\n",
       "  0.605645045045045,\n",
       "  0.6066459459459459,\n",
       "  0.6076468468468468,\n",
       "  0.6086477477477477,\n",
       "  0.6096486486486487,\n",
       "  0.6106495495495495,\n",
       "  0.6116504504504504,\n",
       "  0.6126513513513513,\n",
       "  0.6136522522522522,\n",
       "  0.6146531531531532,\n",
       "  0.615654054054054,\n",
       "  0.6166549549549549,\n",
       "  0.6176558558558558,\n",
       "  0.6186567567567567,\n",
       "  0.6196576576576576,\n",
       "  0.6206585585585586,\n",
       "  0.6216594594594594,\n",
       "  0.6226603603603603,\n",
       "  0.6236612612612612,\n",
       "  0.6246621621621621,\n",
       "  0.6256630630630631,\n",
       "  0.626663963963964,\n",
       "  0.6276648648648648,\n",
       "  0.6286657657657657,\n",
       "  0.6296666666666666,\n",
       "  0.6306675675675675,\n",
       "  0.6316684684684685,\n",
       "  0.6326693693693693,\n",
       "  0.6336702702702702,\n",
       "  0.6346711711711711,\n",
       "  0.635672072072072,\n",
       "  0.636672972972973,\n",
       "  0.6376738738738739,\n",
       "  0.6386747747747747,\n",
       "  0.6396756756756756,\n",
       "  0.6406765765765765,\n",
       "  0.6416774774774774,\n",
       "  0.6426783783783784,\n",
       "  0.6436792792792793,\n",
       "  0.6446801801801801,\n",
       "  0.645681081081081,\n",
       "  0.6466819819819819,\n",
       "  0.6476828828828829,\n",
       "  0.6486837837837838,\n",
       "  0.6496846846846847,\n",
       "  0.6506855855855855,\n",
       "  0.6516864864864864,\n",
       "  0.6526873873873873,\n",
       "  0.6536882882882883,\n",
       "  0.6546891891891892,\n",
       "  0.65569009009009,\n",
       "  0.6566909909909909,\n",
       "  0.6576918918918918,\n",
       "  0.6586927927927928,\n",
       "  0.6596936936936937,\n",
       "  0.6606945945945946,\n",
       "  0.6616954954954954,\n",
       "  0.6626963963963963,\n",
       "  0.6636972972972972,\n",
       "  0.6646981981981982,\n",
       "  0.6656990990990991,\n",
       "  0.6667,\n",
       "  0.6677009009009008,\n",
       "  0.6687018018018017,\n",
       "  0.6697027027027027,\n",
       "  0.6707036036036036,\n",
       "  0.6717045045045045,\n",
       "  0.6727054054054054,\n",
       "  0.6737063063063062,\n",
       "  0.6747072072072071,\n",
       "  0.6757081081081081,\n",
       "  0.676709009009009,\n",
       "  0.6777099099099099,\n",
       "  0.6787108108108107,\n",
       "  0.6797117117117116,\n",
       "  0.6807126126126126,\n",
       "  0.6817135135135135,\n",
       "  0.6827144144144144,\n",
       "  0.6837153153153153,\n",
       "  0.6847162162162161,\n",
       "  0.685717117117117,\n",
       "  0.686718018018018,\n",
       "  0.6877189189189189,\n",
       "  0.6887198198198198,\n",
       "  0.6897207207207207,\n",
       "  0.6907216216216215,\n",
       "  0.6917225225225225,\n",
       "  0.6927234234234234,\n",
       "  0.6937243243243243,\n",
       "  0.6947252252252252,\n",
       "  0.695726126126126,\n",
       "  0.6967270270270269,\n",
       "  0.6977279279279279,\n",
       "  0.6987288288288288,\n",
       "  0.6997297297297297,\n",
       "  0.7007306306306306,\n",
       "  0.7017315315315314,\n",
       "  0.7027324324324324,\n",
       "  0.7037333333333333,\n",
       "  0.7047342342342342,\n",
       "  0.7057351351351351,\n",
       "  0.706736036036036,\n",
       "  0.7077369369369368,\n",
       "  0.7087378378378378,\n",
       "  0.7097387387387387,\n",
       "  0.7107396396396396,\n",
       "  0.7117405405405405,\n",
       "  0.7127414414414414,\n",
       "  0.7137423423423424,\n",
       "  0.7147432432432432,\n",
       "  0.7157441441441441,\n",
       "  0.716745045045045,\n",
       "  0.7177459459459459,\n",
       "  0.7187468468468468,\n",
       "  0.7197477477477477,\n",
       "  0.7207486486486486,\n",
       "  0.7217495495495495,\n",
       "  0.7227504504504504,\n",
       "  0.7237513513513513,\n",
       "  0.7247522522522523,\n",
       "  0.7257531531531531,\n",
       "  0.726754054054054,\n",
       "  0.7277549549549549,\n",
       "  0.7287558558558558,\n",
       "  0.7297567567567567,\n",
       "  0.7307576576576577,\n",
       "  0.7317585585585585,\n",
       "  0.7327594594594594,\n",
       "  0.7337603603603603,\n",
       "  0.7347612612612612,\n",
       "  0.7357621621621622,\n",
       "  0.736763063063063,\n",
       "  0.7377639639639639,\n",
       "  0.7387648648648648,\n",
       "  0.7397657657657657,\n",
       "  0.7407666666666666,\n",
       "  0.7417675675675676,\n",
       "  0.7427684684684684,\n",
       "  0.7437693693693693,\n",
       "  0.7447702702702702,\n",
       "  0.7457711711711711,\n",
       "  0.7467720720720721,\n",
       "  0.747772972972973,\n",
       "  0.7487738738738738,\n",
       "  0.7497747747747747,\n",
       "  0.7507756756756756,\n",
       "  0.7517765765765765,\n",
       "  0.7527774774774775,\n",
       "  0.7537783783783784,\n",
       "  0.7547792792792792,\n",
       "  0.7557801801801801,\n",
       "  0.756781081081081,\n",
       "  0.757781981981982,\n",
       "  0.7587828828828829,\n",
       "  0.7597837837837838,\n",
       "  0.7607846846846846,\n",
       "  0.7617855855855855,\n",
       "  0.7627864864864864,\n",
       "  0.7637873873873874,\n",
       "  0.7647882882882883,\n",
       "  0.7657891891891891,\n",
       "  0.76679009009009,\n",
       "  0.7677909909909909,\n",
       "  0.7687918918918919,\n",
       "  0.7697927927927928,\n",
       "  0.7707936936936937,\n",
       "  0.7717945945945945,\n",
       "  0.7727954954954954,\n",
       "  0.7737963963963963,\n",
       "  0.7747972972972973,\n",
       "  0.7757981981981982,\n",
       "  0.776799099099099,\n",
       "  0.7777999999999999,\n",
       "  0.7788009009009008,\n",
       "  0.7798018018018017,\n",
       "  0.7808027027027027,\n",
       "  0.7818036036036036,\n",
       "  0.7828045045045045,\n",
       "  0.7838054054054053,\n",
       "  0.7848063063063062,\n",
       "  0.7858072072072072,\n",
       "  0.7868081081081081,\n",
       "  0.787809009009009,\n",
       "  0.7888099099099098,\n",
       "  0.7898108108108107,\n",
       "  0.7908117117117116,\n",
       "  0.7918126126126126,\n",
       "  0.7928135135135135,\n",
       "  0.7938144144144144,\n",
       "  0.7948153153153152,\n",
       "  0.7958162162162161,\n",
       "  0.7968171171171171,\n",
       "  0.797818018018018,\n",
       "  0.7988189189189189,\n",
       "  0.7998198198198198,\n",
       "  0.8008207207207206,\n",
       "  0.8018216216216215,\n",
       "  0.8028225225225225,\n",
       "  0.8038234234234234,\n",
       "  0.8048243243243243,\n",
       "  0.8058252252252252,\n",
       "  0.806826126126126,\n",
       "  0.807827027027027,\n",
       "  0.8088279279279279,\n",
       "  0.8098288288288288,\n",
       "  0.8108297297297297,\n",
       "  0.8118306306306305,\n",
       "  0.8128315315315314,\n",
       "  0.8138324324324324,\n",
       "  0.8148333333333333,\n",
       "  0.8158342342342342,\n",
       "  0.8168351351351351,\n",
       "  0.8178360360360359,\n",
       "  0.8188369369369369,\n",
       "  0.8198378378378378,\n",
       "  0.8208387387387387,\n",
       "  0.8218396396396396,\n",
       "  0.8228405405405405,\n",
       "  0.8238414414414413,\n",
       "  0.8248423423423423,\n",
       "  0.8258432432432432,\n",
       "  0.8268441441441441,\n",
       "  0.827845045045045,\n",
       "  0.8288459459459459,\n",
       "  0.8298468468468468,\n",
       "  0.8308477477477477,\n",
       "  0.8318486486486486,\n",
       "  0.8328495495495495,\n",
       "  0.8338504504504504,\n",
       "  0.8348513513513512,\n",
       "  0.8358522522522522,\n",
       "  0.8368531531531531,\n",
       "  0.837854054054054,\n",
       "  0.8388549549549549,\n",
       "  0.8398558558558558,\n",
       "  0.8408567567567568,\n",
       "  0.8418576576576576,\n",
       "  0.8428585585585585,\n",
       "  0.8438594594594594,\n",
       "  0.8448603603603603,\n",
       "  0.8458612612612612,\n",
       "  0.8468621621621621,\n",
       "  0.847863063063063,\n",
       "  0.8488639639639639,\n",
       "  0.8498648648648648,\n",
       "  0.8508657657657657,\n",
       "  0.8518666666666667,\n",
       "  0.8528675675675675,\n",
       "  0.8538684684684684,\n",
       "  0.8548693693693693,\n",
       "  0.8558702702702702,\n",
       "  0.8568711711711711,\n",
       "  0.8578720720720721,\n",
       "  0.8588729729729729,\n",
       "  0.8598738738738738,\n",
       "  0.8608747747747747,\n",
       "  0.8618756756756756,\n",
       "  0.8628765765765766,\n",
       "  0.8638774774774775,\n",
       "  0.8648783783783783,\n",
       "  0.8658792792792792,\n",
       "  0.8668801801801801,\n",
       "  0.867881081081081,\n",
       "  0.868881981981982,\n",
       "  0.8698828828828828,\n",
       "  0.8708837837837837,\n",
       "  0.8718846846846846,\n",
       "  0.8728855855855855,\n",
       "  0.8738864864864865,\n",
       "  0.8748873873873874,\n",
       "  0.8758882882882882,\n",
       "  0.8768891891891891,\n",
       "  0.87789009009009,\n",
       "  0.8788909909909909,\n",
       "  0.8798918918918919,\n",
       "  0.8808927927927928,\n",
       "  0.8818936936936936,\n",
       "  0.8828945945945945,\n",
       "  0.8838954954954954,\n",
       "  0.8848963963963964,\n",
       "  0.8858972972972973,\n",
       "  0.8868981981981982,\n",
       "  0.887899099099099,\n",
       "  0.8888999999999999,\n",
       "  0.8899009009009008,\n",
       "  0.8909018018018018,\n",
       "  0.8919027027027027,\n",
       "  0.8929036036036035,\n",
       "  0.8939045045045044,\n",
       "  0.8949054054054053,\n",
       "  0.8959063063063063,\n",
       "  0.8969072072072072,\n",
       "  0.8979081081081081,\n",
       "  0.8989090090090089,\n",
       "  0.8999099099099098,\n",
       "  0.9009108108108107,\n",
       "  0.9019117117117117,\n",
       "  0.9029126126126126,\n",
       "  0.9039135135135135,\n",
       "  0.9049144144144143,\n",
       "  0.9059153153153152,\n",
       "  0.9069162162162162,\n",
       "  0.9079171171171171,\n",
       "  0.908918018018018,\n",
       "  0.9099189189189189,\n",
       "  0.9109198198198197,\n",
       "  0.9119207207207206,\n",
       "  0.9129216216216216,\n",
       "  0.9139225225225225,\n",
       "  0.9149234234234234,\n",
       "  0.9159243243243242,\n",
       "  0.9169252252252251,\n",
       "  0.9179261261261261,\n",
       "  0.918927027027027,\n",
       "  0.9199279279279279,\n",
       "  0.9209288288288288,\n",
       "  0.9219297297297296,\n",
       "  0.9229306306306305,\n",
       "  0.9239315315315315,\n",
       "  0.9249324324324324,\n",
       "  0.9259333333333333,\n",
       "  0.9269342342342342,\n",
       "  0.927935135135135,\n",
       "  0.928936036036036,\n",
       "  0.9299369369369369,\n",
       "  0.9309378378378378,\n",
       "  0.9319387387387387,\n",
       "  0.9329396396396396,\n",
       "  0.9339405405405404,\n",
       "  0.9349414414414414,\n",
       "  0.9359423423423423,\n",
       "  0.9369432432432432,\n",
       "  0.9379441441441441,\n",
       "  0.938945045045045,\n",
       "  0.9399459459459459,\n",
       "  0.9409468468468468,\n",
       "  0.9419477477477477,\n",
       "  0.9429486486486486,\n",
       "  0.9439495495495495,\n",
       "  0.9449504504504503,\n",
       "  0.9459513513513513,\n",
       "  0.9469522522522522,\n",
       "  0.9479531531531531,\n",
       "  0.948954054054054,\n",
       "  0.9499549549549549,\n",
       "  0.9509558558558558,\n",
       "  0.9519567567567567,\n",
       "  0.9529576576576576,\n",
       "  0.9539585585585585,\n",
       "  0.9549594594594594,\n",
       "  0.9559603603603603,\n",
       "  0.9569612612612612,\n",
       "  0.9579621621621621,\n",
       "  0.958963063063063,\n",
       "  0.9599639639639639,\n",
       "  0.9609648648648648,\n",
       "  0.9619657657657658,\n",
       "  0.9629666666666666,\n",
       "  0.9639675675675675,\n",
       "  0.9649684684684684,\n",
       "  0.9659693693693693,\n",
       "  0.9669702702702702,\n",
       "  0.9679711711711712,\n",
       "  0.968972072072072,\n",
       "  0.9699729729729729,\n",
       "  0.9709738738738738,\n",
       "  0.9719747747747747,\n",
       "  0.9729756756756757,\n",
       "  0.9739765765765765,\n",
       "  0.9749774774774774,\n",
       "  0.9759783783783783,\n",
       "  0.9769792792792792,\n",
       "  0.9779801801801801,\n",
       "  0.9789810810810811,\n",
       "  0.9799819819819819,\n",
       "  0.9809828828828828,\n",
       "  0.9819837837837837,\n",
       "  0.9829846846846846,\n",
       "  0.9839855855855856,\n",
       "  0.9849864864864865,\n",
       "  0.9859873873873873,\n",
       "  0.9869882882882882,\n",
       "  0.9879891891891891,\n",
       "  0.98899009009009,\n",
       "  0.989990990990991,\n",
       "  0.9909918918918919,\n",
       "  0.9919927927927927,\n",
       "  0.9929936936936936,\n",
       "  0.9939945945945945,\n",
       "  0.9949954954954955,\n",
       "  0.9959963963963964,\n",
       "  0.9969972972972972,\n",
       "  0.9979981981981981,\n",
       "  0.998999099099099,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "params = {\n",
    "    \"penalty\" : ['l1','l2','elasticnet'],\n",
    "    \"C\" : [i for i in np.linspace(0.0001,1,1000)]\n",
    "}\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 465 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LogisticRegression(), n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'C': [0.0001, 0.001100900900900901,\n",
       "                                              0.0021018018018018015,\n",
       "                                              0.0031027027027027026,\n",
       "                                              0.004103603603603604,\n",
       "                                              0.005104504504504504,\n",
       "                                              0.006105405405405406,\n",
       "                                              0.0071063063063063064,\n",
       "                                              0.008107207207207206,\n",
       "                                              0.009108108108108108,\n",
       "                                              0.010109009009009007,\n",
       "                                              0.011109909909909909,\n",
       "                                              0.01211...\n",
       "                                              0.015113513513513512,\n",
       "                                              0.016114414414414413,\n",
       "                                              0.017115315315315315,\n",
       "                                              0.018116216216216216,\n",
       "                                              0.019117117117117114,\n",
       "                                              0.020118018018018016,\n",
       "                                              0.021118918918918917,\n",
       "                                              0.02211981981981982,\n",
       "                                              0.02312072072072072,\n",
       "                                              0.02412162162162162,\n",
       "                                              0.02512252252252252,\n",
       "                                              0.02612342342342342,\n",
       "                                              0.027124324324324323,\n",
       "                                              0.028125225225225224,\n",
       "                                              0.029126126126126126, ...],\n",
       "                                        'penalty': ['l1', 'l2', 'elasticnet']},\n",
       "                   verbose=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rndm_cv_logreg = RandomizedSearchCV(estimator = logreg,param_distributions=params , cv = 5,n_iter = 100 ,n_jobs=-1,verbose=True)\n",
    "rndm_cv_logreg.fit(x_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l2', 'C': 0.16624954954954954}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = rndm_cv_logreg.best_params_\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.2783504504504504)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgrg = LogisticRegression(C = 0.2783504504504504 )\n",
    "lgrg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pr = lgrg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8356164383561644\n",
      "[[36  1]\n",
      " [11 25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.97      0.86        37\n",
      "           1       0.96      0.69      0.81        36\n",
      "\n",
      "    accuracy                           0.84        73\n",
      "   macro avg       0.86      0.83      0.83        73\n",
      "weighted avg       0.86      0.84      0.83        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test,y_pr))\n",
    "print(metrics.confusion_matrix(y_test,y_pr))\n",
    "print(metrics.classification_report(y_test,y_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8538812785388128"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgrg.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knc = KNeighborsClassifier()\n",
    "knc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863013698630137\n",
      "[[35  2]\n",
      " [ 8 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.88        37\n",
      "           1       0.93      0.78      0.85        36\n",
      "\n",
      "    accuracy                           0.86        73\n",
      "   macro avg       0.87      0.86      0.86        73\n",
      "weighted avg       0.87      0.86      0.86        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "print(metrics.confusion_matrix(y_test,y_pred))\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.908675799086758"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knc.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_neighbors':[i for i in range(1,30,2)],\n",
    "    'weights':['uniform','distance'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:278: UserWarning: The total space of parameters 30 is smaller than n_iter=100. Running 30 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=KNeighborsClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'n_neighbors': [1, 3, 5, 7, 9, 11, 13,\n",
       "                                                        15, 17, 19, 21, 23, 25,\n",
       "                                                        27, 29],\n",
       "                                        'weights': ['uniform', 'distance']},\n",
       "                   verbose=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomcv_knc = RandomizedSearchCV(knc,param_distributions=params,n_jobs=-1,cv=10,n_iter=100,verbose=True)\n",
    "randomcv_knc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc_best = randomcv_knc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=7, weights='distance')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='ball_tree', n_neighbors=13, weights='distance')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knc_clf = KNeighborsClassifier(algorithm='ball_tree', n_neighbors=13, weights='distance')\n",
    "knc_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493150684931506"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knc_clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knc_clf.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = dtc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7671232876712328\n",
      "[[29  8]\n",
      " [ 9 27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77        37\n",
      "           1       0.77      0.75      0.76        36\n",
      "\n",
      "    accuracy                           0.77        73\n",
      "   macro avg       0.77      0.77      0.77        73\n",
      "weighted avg       0.77      0.77      0.77        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test,y_p))\n",
    "print(metrics.confusion_matrix(y_test,y_p))\n",
    "print(metrics.classification_report(y_test,y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=DecisionTreeClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19],\n",
       "                                        'max_features': ['sqrt', 'auto', 'log2',\n",
       "                                                         None],\n",
       "                                        'min_samples_leaf': [1, 2, 3, 4, 5, 6,\n",
       "                                                             7, 8, 9, 10, 11,\n",
       "                                                             12, 13, 14, 15, 16,\n",
       "                                                             17, 18, 19, 20, 21,\n",
       "                                                             22, 23, 24, 25, 26,\n",
       "                                                             27, 28, 29, 30, ...],\n",
       "                                        'min_samples_split': [2, 3, 4, 5, 6, 7,\n",
       "                                                              8, 9, 10, 11, 12,\n",
       "                                                              13, 14, 15, 16,\n",
       "                                                              17, 18, 19, 20,\n",
       "                                                              21, 22, 23, 24,\n",
       "                                                              25, 26, 27, 28,\n",
       "                                                              29, 30, 31, ...],\n",
       "                                        'splitter': ['best', 'random']},\n",
       "                   verbose=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "params = {\n",
    "    'max_features' : ['sqrt','auto','log2',None],\n",
    "    'criterion' : ['gini','entropy'],\n",
    "    'max_depth':[i for i in range(2,20,1)],\n",
    "    'splitter':['best','random'],\n",
    "    'min_samples_split' : [i for i in range(2,50,1)],\n",
    "    'min_samples_leaf' : [i for i in range(1,50,1)]\n",
    "}\n",
    "randomcv_dtc = RandomizedSearchCV(dtc,param_distributions=params,n_jobs=-1,cv=10,n_iter=100,verbose=True)\n",
    "randomcv_dtc.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_best = randomcv_dtc.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=17, max_features='sqrt',\n",
       "                       min_samples_split=12)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_clf = DecisionTreeClassifier(criterion=dtc_best['criterion'],\n",
    "                                           min_samples_leaf=dtc_best['min_samples_leaf'],\n",
    "                                           min_samples_split=dtc_best['min_samples_split'],\n",
    "                                           max_features=dtc_best['max_features'],\n",
    "                                splitter=dtc_best['splitter'],max_depth=dtc_best['max_depth'])\n",
    "dtc_clf.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Hyperparameter Tuning :- \n",
      "Training Score :-  0.9543\n",
      "Testing Score :-  0.8082\n",
      "\n",
      "Now the model looks more Generalized !!!\n"
     ]
    }
   ],
   "source": [
    "print(\"After Hyperparameter Tuning :- \")\n",
    "print(\"Training Score :- \",round(dtc_clf.score(x_train,y_train),4))\n",
    "print(\"Testing Score :- \",round(dtc_clf.score(x_test,y_test),4))\n",
    "print()\n",
    "print(\"Now the model looks more Generalized !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = rfc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8904109589041096\n",
      "[[35  2]\n",
      " [ 6 30]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90        37\n",
      "           1       0.94      0.83      0.88        36\n",
      "\n",
      "    accuracy                           0.89        73\n",
      "   macro avg       0.90      0.89      0.89        73\n",
      "weighted avg       0.90      0.89      0.89        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test,y_pre))\n",
    "print(metrics.confusion_matrix(y_test,y_pre))\n",
    "print(metrics.classification_report(y_test,y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "                'n_estimators': [int(x) for x in np.linspace(start=100,stop=3000,num=30)],\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_leaf': [int(x) for x in range(1, 25, 1)],\n",
    "                'min_samples_split': [int(x) for x in range(2, 50, 1)],\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                'max_depth': [int(x) for x in range(1,31,2)]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_grid = RandomizedSearchCV(estimator=rfc, param_distributions=params, n_iter=100, cv=10, n_jobs=-1, verbose=2,\n",
    "                                          random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   22.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  9.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=RandomForestClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [1, 3, 5, 7, 9, 11, 13, 15,\n",
       "                                                      17, 19, 21, 23, 25, 27,\n",
       "                                                      29],\n",
       "                                        'max_features': ['sqrt', 'log2', None],\n",
       "                                        'min_samples_leaf': [1, 2, 3, 4, 5, 6,\n",
       "                                                             7, 8, 9, 10, 11,\n",
       "                                                             12, 13, 14, 15, 16,\n",
       "                                                             17, 18, 19, 20, 21,\n",
       "                                                             22, 23, 24],\n",
       "                                        'min_samples_split': [2, 3, 4, 5, 6, 7,\n",
       "                                                              8, 9, 10, 11, 12,\n",
       "                                                              13, 14, 15, 16,\n",
       "                                                              17, 18, 19, 20,\n",
       "                                                              21, 22, 23, 24,\n",
       "                                                              25, 26, 27, 28,\n",
       "                                                              29, 30, 31, ...],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000, 1100, 1200,\n",
       "                                                         1300, 1400, 1500, 1600,\n",
       "                                                         1700, 1800, 1900, 2000,\n",
       "                                                         2100, 2200, 2300, 2400,\n",
       "                                                         2500, 2600, 2700, 2800,\n",
       "                                                         2900, 3000]},\n",
       "                   random_state=101, verbose=2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 2400, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 29, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "rfc_best = rfc_grid.best_params_\n",
    "print(rfc_best)\n",
    "            # print(xgbc_best.get_booster().feature_names)\n",
    "rfc_clf = RandomForestClassifier(criterion=rfc_best['criterion'], n_estimators=rfc_best['n_estimators'],\n",
    "                                           min_samples_leaf=rfc_best['min_samples_leaf'],\n",
    "                                           min_samples_split=rfc_best['min_samples_split'],\n",
    "                                           max_features=rfc_best['max_features'], max_depth=rfc_best['max_depth'])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=29, max_features=None, min_samples_leaf=4,\n",
       "                       min_samples_split=5, n_estimators=2400)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Hyperparameter Tuning :- \n",
      "Training Score :-  0.9635\n",
      "Testing Score :-  0.8493\n",
      "\n",
      "Now the model looks more Generalized !!!\n"
     ]
    }
   ],
   "source": [
    "print(\"After Hyperparameter Tuning :- \")\n",
    "print(\"Training Score :- \",round(rfc_clf.score(x_train,y_train),4))\n",
    "print(\"Testing Score :- \",round(rfc_clf.score(x_test,y_test),4))\n",
    "print()\n",
    "print(\"Now the model looks more Generalized !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pr1 = gbc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8767123287671232\n",
      "[[33  4]\n",
      " [ 5 31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88        37\n",
      "           1       0.89      0.86      0.87        36\n",
      "\n",
      "    accuracy                           0.88        73\n",
      "   macro avg       0.88      0.88      0.88        73\n",
      "weighted avg       0.88      0.88      0.88        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,y_pr1))\n",
    "print(metrics.confusion_matrix(y_test,y_pr1))\n",
    "print(metrics.classification_report(y_test,y_pr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(gbc.score(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'loss':['log_loss','exponential'],\n",
    "    'learning_rate' :[i for i in np.linspace(0.001,1,50)],\n",
    "    'n_estimators':[i for i in range(1,301,20)],\n",
    "    'min_samples_leaf': [int(x) for x in range(1, 25, 1)],\n",
    "    'min_samples_split': [int(x) for x in range(2, 50, 1)],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': [int(x) for x in range(1,31,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_grid = RandomizedSearchCV(estimator=gbc, param_distributions=params, n_iter=100, cv=10, n_jobs=-1, verbose=2,\n",
    "                                          random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   13.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=GradientBoostingClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'learning_rate': [0.001,\n",
       "                                                          0.021387755102040818,\n",
       "                                                          0.041775510204081635,\n",
       "                                                          0.062163265306122456,\n",
       "                                                          0.08255102040816327,\n",
       "                                                          0.10293877551020408,\n",
       "                                                          0.12332653061224491,\n",
       "                                                          0.1437142857142857,\n",
       "                                                          0.16410204081632654,\n",
       "                                                          0.18448979591836737,\n",
       "                                                          0.20487755102040817,\n",
       "                                                          0.225265306122449,\n",
       "                                                          0...\n",
       "                                        'max_depth': [1, 3, 5, 7, 9, 11, 13, 15,\n",
       "                                                      17, 19, 21, 23, 25, 27,\n",
       "                                                      29],\n",
       "                                        'max_features': ['sqrt', 'log2', None],\n",
       "                                        'min_samples_leaf': [1, 2, 3, 4, 5, 6,\n",
       "                                                             7, 8, 9, 10, 11,\n",
       "                                                             12, 13, 14, 15, 16,\n",
       "                                                             17, 18, 19, 20, 21,\n",
       "                                                             22, 23, 24],\n",
       "                                        'min_samples_split': [2, 3, 4, 5, 6, 7,\n",
       "                                                              8, 9, 10, 11, 12,\n",
       "                                                              13, 14, 15, 16,\n",
       "                                                              17, 18, 19, 20,\n",
       "                                                              21, 22, 23, 24,\n",
       "                                                              25, 26, 27, 28,\n",
       "                                                              29, 30, 31, ...],\n",
       "                                        'n_estimators': [1, 21, 41, 61, 81, 101,\n",
       "                                                         121, 141, 161, 181,\n",
       "                                                         201, 221, 241, 261,\n",
       "                                                         281]},\n",
       "                   random_state=101, verbose=2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 201, 'min_samples_split': 44, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': 15, 'loss': 'exponential', 'learning_rate': 0.12332653061224491}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.12332653061224491,\n",
       "                           loss='exponential', max_depth=15,\n",
       "                           max_features='sqrt', min_samples_leaf=5,\n",
       "                           min_samples_split=44, n_estimators=201)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_best = gbc_grid.best_params_\n",
    "print(gbc_best)\n",
    "gbc_clf = GradientBoostingClassifier(loss=gbc_best['loss'],learning_rate=gbc_best['learning_rate'],\n",
    "                                    n_estimators=gbc_best['n_estimators'],min_samples_split=gbc_best['min_samples_split'],\n",
    "                                    min_samples_leaf=gbc_best['min_samples_leaf'],max_features=gbc_best['max_features'],max_depth=gbc_best['max_depth'])\n",
    "gbc_clf.fit(x_train,y_train)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score :-  1.0\n",
      "Testing Score :-  0.9041\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Score :- \",round(gbc_clf.score(x_train,y_train),4))\n",
    "print(\"Testing Score :- \",round(gbc_clf.score(x_test,y_test),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgbc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8356164383561644\n",
      "[[30  7]\n",
      " [ 5 31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83        37\n",
      "           1       0.82      0.86      0.84        36\n",
      "\n",
      "    accuracy                           0.84        73\n",
      "   macro avg       0.84      0.84      0.84        73\n",
      "weighted avg       0.84      0.84      0.84        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "print(metrics.confusion_matrix(y_test,y_pred))\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(xgbc.score(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "                \"learning_rate\": [float(i) for i in np.linspace(0.001,0.2,20)],\n",
    "                #\"max_depth\": [3, 4, 5, 6, 8, 10, 12, 15],\n",
    "                \"min_child_weight\": [1, 3, 5, 7],\n",
    "                \"gamma\": [float(i) for i in np.linspace(0.1,1,10)],\n",
    "                \"colsample_bytree\": [0.2,0.3, 0.4, 0.5, 0.7,0.8,0.9]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  40 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=6)]: Done 1000 out of 1000 | elapsed:    9.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10,\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1, gamma=0,\n",
       "                                           gpu_id=-1, importance_type='gain',\n",
       "                                           interaction_constraints='',\n",
       "                                           learning_rate=0.300000012,\n",
       "                                           max_delta_step=0, max_depth=6,\n",
       "                                           min_child_weight=1, missing=nan,\n",
       "                                           monotone_constraints='()',\n",
       "                                           n_estimators=100, n_jobs=0,\n",
       "                                           num_p...\n",
       "                                                          0.04289473684210526,\n",
       "                                                          0.05336842105263158,\n",
       "                                                          0.0638421052631579,\n",
       "                                                          0.07431578947368421,\n",
       "                                                          0.08478947368421053,\n",
       "                                                          0.09526315789473684,\n",
       "                                                          0.10573684210526316,\n",
       "                                                          0.11621052631578947,\n",
       "                                                          0.1266842105263158,\n",
       "                                                          0.1371578947368421,\n",
       "                                                          0.14763157894736842,\n",
       "                                                          0.15810526315789475,\n",
       "                                                          0.16857894736842105,\n",
       "                                                          0.17905263157894735,\n",
       "                                                          0.18952631578947368,\n",
       "                                                          0.2],\n",
       "                                        'min_child_weight': [1, 3, 5, 7]},\n",
       "                   random_state=75, verbose=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_rndm = RandomizedSearchCV(estimator=xgbc, param_distributions=params, cv=10, n_iter=100, n_jobs=6, verbose=True, random_state=75)\n",
    "xgbc_rndm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_child_weight': 5, 'learning_rate': 0.05336842105263158, 'gamma': 0.9, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "xgbc_best = xgbc_rndm.best_params_\n",
    "print(xgbc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, gamma=0.9, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.05336842105263158, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clf = XGBClassifier(learning_rate=xgbc_best['learning_rate'],min_child_weight=xgbc_best['min_child_weight'],gamma=xgbc_best['gamma'],colsample_bytree=xgbc_best['colsample_bytree'])\n",
    "final_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8356164383561644"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9497716894977168"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clf.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8493150684931506\n",
      "[[37  0]\n",
      " [11 25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87        37\n",
      "           1       1.00      0.69      0.82        36\n",
      "\n",
      "    accuracy                           0.85        73\n",
      "   macro avg       0.89      0.85      0.85        73\n",
      "weighted avg       0.88      0.85      0.85        73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "print(metrics.confusion_matrix(y_test,y_pred))\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8995433789954338\n"
     ]
    }
   ],
   "source": [
    "print(svc.score(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "                'kernel': ['rbf', 'linear', 'sigmoid', 'poly'],\n",
    "                'degree': [i for i in range(2, 20, 1)],\n",
    "                'gamma': ['scale', 'auto'],\n",
    "                'decision_function_shape': ['ovr', 'ovo']\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 288 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEGION\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:278: UserWarning: The total space of parameters 288 is smaller than n_iter=1000. Running 288 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2160 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=SVC(), n_iter=1000, n_jobs=-1,\n",
       "                   param_distributions={'decision_function_shape': ['ovr',\n",
       "                                                                    'ovo'],\n",
       "                                        'degree': [2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
       "                                                   11, 12, 13, 14, 15, 16, 17,\n",
       "                                                   18, 19],\n",
       "                                        'gamma': ['scale', 'auto'],\n",
       "                                        'kernel': ['rbf', 'linear', 'sigmoid',\n",
       "                                                   'poly']},\n",
       "                   random_state=100, verbose=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_randomcv = RandomizedSearchCV(estimator=svc, param_distributions=params, n_iter=1000,cv=10, n_jobs=-1, random_state=100, verbose=True)\n",
    "svc_randomcv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': 'rbf',\n",
       " 'gamma': 'scale',\n",
       " 'degree': 2,\n",
       " 'decision_function_shape': 'ovr'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_best = svc_randomcv.best_params_\n",
    "svc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(degree=2)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf = SVC(kernel=svc_best['kernel'], degree=svc_best['degree'], gamma=svc_best['gamma'], decision_function_shape=svc_best['decision_function_shape'])\n",
    "svc_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8995433789954338"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493150684931506"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
